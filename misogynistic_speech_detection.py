# -*- coding: utf-8 -*-
"""Misogynistic Speech Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/127zoQI2yhVjeV1L323BSD1_l-KxBWzeJ
"""

# -*- coding: utf-8 -*-

import numpy as np
import pandas as pd
import logging
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

import tensorflow as tf
from tensorflow.keras import backend as K
from tensorflow.keras.callbacks import EarlyStopping

# !pip install transformers
import transformers
from transformers import BertTokenizer, TFBertModel, TFBertForSequenceClassification


!git clone https://github.com/dianapaula19/ami-ai-project/tree/master/data.git
twitter_data = pd.read_csv('/content/train.csv')
twitter_data.head()
twitter_data.describe()
twitter_data.label.value_counts()


# !pip install biterm pyLDAvis --quiet

# Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)


# Load & Preprocess Data

def load_and_describe_data(csv_path: str) -> pd.DataFrame:
    logger.info(f"Loading data from: {csv_path}")
    df = pd.read_csv(csv_path)
    logger.info(f"Data shape: {df.shape}")
    logger.info(f"Data columns: {df.columns.to_list()}")
    logger.info(f"\n{df.head()}")
    return df


def preprocess_text_data(df: pd.DataFrame, text_col: str = "text") -> pd.DataFrame:
    """
    Perform any text cleaning/preprocessing needed.
    """
    # Example: remove line breaks, strip whitespace, etc.
    logger.info("Preprocessing text data...")
    df[text_col] = df[text_col].astype(str).apply(
        lambda x: x.replace("\n", " ").replace("\t", " ").strip()
    )
    return df



# BERT Model Construction

# load the tokenizer and model, note that several different were experimented with:
tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
bert_model = TFBertModel.from_pretrained('bert-base-cased')

def create_classification_model(
    max_length: int,
    hidden_size: int = 256,
    train_layers: int = -1,
    optimizer: tf.keras.optimizers.Optimizer = tf.keras.optimizers.Adam()
) -> tf.keras.Model:
    """
    Create a BERT-based classification model for misogynistic tweet detection.
    Demonstrates layer freezing and a custom dense classification head.

    :param max_length: The max length (in tokens) for input.
    :param hidden_size: The size of the hidden dense layer.
    :param train_layers: Number of last BERT layers to unfreeze; -1 means all trainable.
    :param optimizer: Keras/TensorFlow optimizer.
    :return: A compiled Keras Model.
    """
    global bert_model
    logger.info("Building classification model with BERT backbone...")

    # Token inputs for BERT
    input_ids = tf.keras.layers.Input(
        shape=(max_length,), dtype=tf.int32, name='input_ids_layer'
    )
    token_type_ids = tf.keras.layers.Input(
        shape=(max_length,), dtype=tf.int32, name='token_type_ids_layer'
    )
    attention_mask = tf.keras.layers.Input(
        shape=(max_length,), dtype=tf.int32, name='attention_mask_layer'
    )

    bert_inputs = {
        'input_ids': input_ids,
        'token_type_ids': token_type_ids,
        'attention_mask': attention_mask
    }

    # If BERT isn't loaded, load it now
    if bert_model is None:
        bert_model = TFBertModel.from_pretrained('bert-base-cased')  # or other Italian model

    # Freeze or unfreeze layers
    if train_layers != -1:
        logger.info(f"Freezing all but last {train_layers} BERT layers...")
        retrain_layers = []
        for i in range(train_layers):
            layer_code = '_' + str(11 - i)
            retrain_layers.append(layer_code)
        for w in bert_model.weights:
            if not any([x in w.name for x in retrain_layers]):
                w._trainable = False

    # BERT outputs
    bert_out = bert_model(bert_inputs)[0]  # shape: (batch, seq_len, hidden_dim=768)

    # Classification head
    dense_inputs = tf.keras.layers.Reshape((768 * max_length, ))(bert_out)
    dense_inputs = tf.keras.layers.LayerNormalization()(dense_inputs)
    hidden = tf.keras.layers.Dense(hidden_size, name='hidden_layer')(dense_inputs)
    hidden = tf.keras.layers.Dropout(rate=0.1)(hidden)
    classification = tf.keras.layers.Dense(
        1, activation='sigmoid', name='classification_layer'
    )(hidden)

    # Build final model
    classification_model = tf.keras.Model(
        inputs=[input_ids, token_type_ids, attention_mask],
        outputs=[classification]
    )
    classification_model.compile(
        optimizer=optimizer,
        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),
        metrics=['accuracy']
    )
    logger.info(classification_model.summary())
    return classification_model


# Model Training & Prediction
def train_and_evaluate_model(
    model: tf.keras.Model,
    x_train: list,
    y_train: np.ndarray,
    x_val: list,
    y_val: np.ndarray,
    epochs: int = 4,
    batch_size: int = 8
):
    """
    Train the model and evaluate on validation set.

    :param model: The compiled Keras model.
    :param x_train: List of tokenized input [input_ids, token_type_ids, attention_mask] for training.
    :param y_train: Training labels.
    :param x_val: Tokenized inputs for validation.
    :param y_val: Validation labels.
    :param epochs: Number of epochs.
    :param batch_size: Batch size.
    """
    logger.info("Starting model training...")
    # Example: EarlyStopping
    early_stop = EarlyStopping(
        monitor='val_loss', patience=2, restore_best_weights=True
    )
    history = model.fit(
        x_train, y_train,
        validation_data=(x_val, y_val),
        epochs=epochs,
        batch_size=batch_size,
        callbacks=[early_stop]
    )
    return history


def predict_and_evaluate(model: tf.keras.Model, x_test: list, y_test: np.ndarray):
    """
    Generate predictions on the test set and evaluate the model's performance.

    :param model: Trained Keras model.
    :param x_test: List of tokenized inputs for test.
    :param y_test: Ground-truth labels for test data.
    """
    logger.info("Evaluating model on test set...")
    test_pred = model.predict(x_test, batch_size=8)
    pred_0or1 = np.array([1 if i > 0.5 else 0 for i in test_pred])

    logger.info('Confusion Matrix:')
    conf_matrix = tf.math.confusion_matrix(y_test, pred_0or1, num_classes=2)
    logger.info(f"\n{conf_matrix.numpy()}")

    from sklearn.metrics import classification_report, accuracy_score
    logger.info("Classification Report:")
    logger.info("\n" + classification_report(y_test, pred_0or1, digits=4))
    logger.info("Accuracy: %.4f" % accuracy_score(y_test, pred_0or1))

plot_roc_pr_curves(y_test, y_prob, show_plot=True)



# Visualization: Confusion Matrix

def plot_confusion_matrix(matrix_array: np.ndarray):
    """
    Plot confusion matrix using Seaborn heatmap.

    :param matrix_array: 2D confusion matrix array.
    """
    df_cm = pd.DataFrame(matrix_array, range(matrix_array.shape[0]), range(matrix_array.shape[1]))
    sns.set(font_scale=1.4)
    sns.heatmap(df_cm, annot=True, annot_kws={"size": 16}, fmt="d")
    plt.show()

#  ROC & Additional Metrics

from sklearn.metrics import (
    roc_curve, auc,
    precision_recall_curve, average_precision_score
)

def plot_roc_pr_curves(y_true, y_prob, show_plot=True):
    """
    Plot ROC and Precision-Recall curves, and print out AUC/AP scores.
    Typically used for binary classification.

    :param y_true: Ground truth labels (0 or 1).
    :param y_prob: Predicted probabilities (floats in [0,1]).
    :param show_plot: If True, display the plots.
    """
    # ROC Curve & AUC
    fpr, tpr, roc_thresholds = roc_curve(y_true, y_prob)
    roc_auc = auc(fpr, tpr)

    # Precision-Recall Curve & AP
    precision, recall, pr_thresholds = precision_recall_curve(y_true, y_prob)
    avg_precision = average_precision_score(y_true, y_prob)

    print("==== Additional Metrics ====")
    print(f"ROC AUC: {roc_auc:.4f}")
    print(f"Average Precision (AP): {avg_precision:.4f}")

    if show_plot:
        import matplotlib.pyplot as plt

        fig, axes = plt.subplots(1, 2, figsize=(12, 5))

        # --- Subplot 1: ROC Curve ---
        axes[0].plot(fpr, tpr, label=f"ROC curve (AUC = {roc_auc:.2f})")
        axes[0].plot([0, 1], [0, 1], 'r--', label="Random guess")
        axes[0].set_xlabel("False Positive Rate")
        axes[0].set_ylabel("True Positive Rate")
        axes[0].set_title("ROC Curve")
        axes[0].legend(loc="lower right")

        # --- Subplot 2: PR Curve ---
        axes[1].plot(recall, precision, label=f"PR curve (AP = {avg_precision:.2f})")
        axes[1].set_xlabel("Recall")
        axes[1].set_ylabel("Precision")
        axes[1].set_title("Precision-Recall Curve")
        axes[1].legend(loc="lower left")

        plt.show()


# Topic Modeling Part (Biterm / BTM)

def run_btm_topic_modeling():
    """
    Demonstrates usage of BTM for topic modeling. Adjust as needed.
    """
    from biterm.utility import vec_to_biterms
    from biterm.btm import oBTM
    import pyLDAvis

    logger.info("Example: BTM Topic Modeling with sample data...")

    # Dummy example reads first 50 lines from train.csv
    # texts = open('/content/train.csv').read().splitlines()[:50]

    # adapt to real data
    texts = [
        "Questo Ã¨ un tweet di esempio.",
        "Alcuni tweet potrebbero contenere linguaggio offensivo.",
        # ...
    ]

    from sklearn.feature_extraction.text import CountVectorizer
    vec = CountVectorizer(stop_words='english')
    X = vec.fit_transform(texts).toarray()

    vocab = np.array(vec.get_feature_names_out())
    biterms = vec_to_biterms(X)

    # Create and fit BTM
    btm = oBTM(num_topics=5, V=vocab)
    topics = btm.fit_transform(biterms, iterations=50)

    logger.info("Topic modeling completed. Summaries can be printed or visualized.")
    # For a full interactive visualization, you could do:
    # vis = pyLDAvis.prepare(btm.phi_wz.T, topics, np.count_nonzero(X, axis=1), vocab, np.sum(X, axis=0))
    # pyLDAvis.save_html(vis, 'online_btm.html')


# Main Execution Flow

if __name__ == "__main__":

    # Basic describe
    logger.info(f"\n{twitter_data.describe()}")

    # Convert label if needed, or ensure numeric
    # e.g., if label is '0'/'1' strings, convert them to int
    # twitter_data['label'] = twitter_data['label'].astype(int)

    # Shuffle data
    model_training = twitter_data.sample(frac=1, random_state=42).reset_index(drop=True)

    # Preprocess text
    model_training = preprocess_text_data(model_training, text_col='text')

    # e.g. Split into train/test
    training_data = model_training.iloc[:4000, 0:3]
    testing_data = model_training.iloc[4001:, :]

    logger.info(f"Training data shape: {training_data.shape}")
    logger.info(f"Testing data shape: {testing_data.shape}")

    # Build features for BERT
    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')
    max_length = 75

    def tokenize_texts(texts):
        return tokenizer(
            texts.tolist(),
            max_length=max_length,
            truncation=True,
            padding='max_length',
            return_tensors='tf'
        )

    x_train_tokenized = tokenize_texts(training_data['text'])
    y_train = training_data['label'].values
    x_test_tokenized = tokenize_texts(testing_data['text'])
    y_test = testing_data['label'].values

    # Create & train model
    classification_model = create_classification_model(
        max_length=max_length,
        train_layers=2,  # freeze all but last 2 layers, example
        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5)
    )

    history = train_and_evaluate_model(
        classification_model,
        [x_train_tokenized.input_ids,
         x_train_tokenized.token_type_ids,
         x_train_tokenized.attention_mask],
        y_train,
        [x_test_tokenized.input_ids,
         x_test_tokenized.token_type_ids,
         x_test_tokenized.attention_mask],
        y_test,
        epochs=4,
        batch_size=8
    )

    # Evaluate on test set
    predict_and_evaluate(
        classification_model,
        [x_test_tokenized.input_ids,
         x_test_tokenized.token_type_ids,
         x_test_tokenized.attention_mask],
        y_test
    )

    # Confusion matrix example (manually computed array or from predictions)
    array = [[494, 46],
             [99, 360]]
    plot_confusion_matrix(np.array(array))